{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input query\n",
    "\n",
    "query = str(input())\n",
    "\n",
    "\n",
    "query = text_formatting(query)\n",
    "query = query.split(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f330b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.??\",encoding = \"utf-8\")    as f:\n",
    "    \n",
    "vocabulary = f.split(\"\\n\")                         # Splitting each word in a new Row, how?\n",
    "\n",
    "term_id_list = list()                                       # Creating a list for words-indexes\n",
    "doc_list = list()                                           # Creating a list of matching documents\n",
    "\n",
    "\n",
    "for word in query:                                 # Searching for each word in the query into the vocabulary  \n",
    "    \n",
    "    \n",
    "    # If the word is in the vocabulary\n",
    "    if w in vocabulary:\n",
    "        \n",
    "        # Get the vocabulary index for the word\n",
    "        term_id = vocabulary.index(w)\n",
    "        \n",
    "        # Append it to the list of term_ids\n",
    "        term_id_list.append(term_id)\n",
    "    \n",
    "    # Solo i doc con tutte le parole? altrimenti non serve\n",
    "    else:\n",
    "        term_id_list = list()\n",
    "        break\n",
    "                \n",
    "\n",
    "\n",
    "# Get list of documetns containing the words (term_ids) just found\n",
    "for term_id in term_id_list:\n",
    "    if term_id in 'synopsis_index.json': #come si puo mettere?\n",
    "        doc_list.append(dictionary[term_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the intersection of the sets of documents corresponding to each term_id we have found\n",
    "\n",
    "if len(doc_list) > 0:\n",
    "    selected_docs = set(doc_list[0])\n",
    "\n",
    "    for l in doc_list:\n",
    "        selected_docs = selected_docs.intersection(set(l))\n",
    "else:\n",
    "    selected_docs = list()\n",
    "    \n",
    "selected_docs = list(selected_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69581aa6",
   "metadata": {},
   "source": [
    "we are now going to implement a scoring system. The general principle is to compute the distance between the query and each one of the documents. This is done by creating a vector for each one of them. Each component of the vectors corresponds to a word. In the query vector, if the word is contained, the component will be $1$, otherwise it will be $0$. In the documents vectors, the components are the $TFIDF = TF \\cdot IDF$ of each word in the given document.</p>\n",
    "\n",
    "The TF (term frequency) is defined as:\n",
    "\n",
    "TF = nw/Nw\n",
    "\n",
    "where nw is the number of occurences of the word in document, and Nw the total number of words in the document.\n",
    "\n",
    "while the IDF (inverse document frequency) is:\n",
    "\n",
    "IDF = log(Nd/nd)\n",
    "\n",
    "where Nd is the total number of documents and nd the number of documents containing the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file = open(\"vocabulary.json\", \"r\", \n",
    "                       encoding = \"utf-8\")                     # Open the vocabulary file\n",
    "vocabulary = vocabulary_file.read()                            # Reading \n",
    "vocabulary = vocabulary.split(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "# For every tsv\n",
    "# Open file\n",
    "    file = open(file_path  \".tsv\", \"r\", encoding = \"utf-8\")#come li prendo?\n",
    "\n",
    "    # Read entry\n",
    "    entry = file.read()\n",
    "    entry = entry.split(\"\\t\")\n",
    "\n",
    "    # Get description and title\n",
    "    description = entry[10]\n",
    "    title = entry[0]\n",
    "\n",
    "    # Merge in a single string variable the title and the description\n",
    "    des_tit = description + \" \" + title\n",
    "    des_tit = des_tit.split(\" \")\n",
    "\n",
    "    # Compute TFIDF\n",
    "    \n",
    "    # Def a counter \n",
    "    counter = Counter(des_tit)\n",
    "    \n",
    "    # For every word in the description and title\n",
    "    for word in set(des_tit):\n",
    "        \n",
    "        # Compute the term frequency in the given document\n",
    "        tf = (counter[word]/len(des_tit))\n",
    "        \n",
    "        # Get the term_id from the vocabulary\n",
    "        term_id = vocabulary.index(word)\n",
    "    \n",
    "        # Compute the inverse document frequency\n",
    "        N = 19124 #how much file?  \n",
    "        n = len(dizionario[term_id]) #come la prendo questa?\n",
    "        idf = math.log10(N/n)\n",
    "    \n",
    "        # Compute the TFIDF\n",
    "        tfidf = tf*idf\n",
    "\n",
    "        # Second inverted index\n",
    "        \n",
    "        dictionary2= dict()\n",
    "        if (term_id not in dictionary2):\n",
    "            dictionary2[term_id] = [(doc_index, tfidf)]      \n",
    "        else (term_id in dictionary2):                       \n",
    "            dictionary2[term_id].append((doc_index, tfidf))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted index in a file\n",
    "\n",
    "file = open(\"inverted_index2.json\", \"w\")  #formato?\n",
    "\n",
    "for key in dictionary2:\n",
    "    file.write(str(key) + \": \" + str(dictionary2[key]) + \"\\n\")\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484903e",
   "metadata": {},
   "source": [
    "NOW:  fill the query vector components with 1 if the word corresponding to the component is present in the query, or 0 if it's not present. We will also fill the documents vectors components with the TFIDF of each term. Then we will compute the similarity between the query vector and each one of the document vectors as the cosine:\n",
    "\n",
    " cos(theta) = (q*d)/(|q|*|d|)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9779e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file = open(\"vocabulary.txt\", \"r\", \n",
    "                       encoding = \"utf-8\")          \n",
    "vocabulary = vocabulary_file.read()                 \n",
    "vocabulary = vocabulary.split(\"\\n\")\n",
    "\n",
    "# Create a vector for the query\n",
    "\n",
    "v_query = [0]*(len('quante parole ci sono nel dizionario'+1)      # Initialize the components to 0\n",
    "\n",
    "\n",
    "# To store the vectors for the documents\n",
    "v_docs = list()\n",
    "\n",
    "for doc_index in range(19124):       # Initialize the components to 0\n",
    "    v_docs.append([])\n",
    "    for j in range('quante parole ci sono nel dizionario'+1):\n",
    "        v_docs[doc_index].append(0)\n",
    "\n",
    "\n",
    "# Fill the query vector with 1 (if a word is present) or leave it to 0 (if a word is not present)        \n",
    "for word in query:\n",
    "    if word in vocabulary:\n",
    "        term_id = vocabulary.index(word)\n",
    "        v_query[int(term_id)] = 1\n",
    "\n",
    "# Fill the documents vectors with the TFIDFs for the words        \n",
    "for term_id in dictionary2:\n",
    "    for tpl in dictionary2[term_id]:\n",
    "        doc_index = int(tpl[0])\n",
    "        tfidf = float(tpl[1])\n",
    "        v_docs[doc_index][int(term_id)] = tfidf\n",
    "\n",
    "# Close vocabulary file\n",
    "vocabulary_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4312ca0",
   "metadata": {},
   "source": [
    "Dalla regia mi dicono (Benedetta): store the cosines  in a heap structure.\n",
    "Computationally more efficient than it would be by using a simple list.\n",
    "Ok se posso usare le librerie dovrebbe essere facile, atrimenti no idea.\n",
    "Copiato spudoratamente dal web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60091cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "a_query = np.array(v_query)\n",
    "\n",
    "# Create a heap of scores and a dictionary of scores\n",
    "heap = list()\n",
    "heapq.heapify(heap)\n",
    "scores_dictionary = dict()\n",
    "\n",
    "# For every document\n",
    "for doc_index in range(19184):\n",
    "    \n",
    "    \n",
    "    a_doc = np.array(v_docs[doc_index])\n",
    "    \n",
    "    # Compute the cosine of the angle between the query vector and the document vector\n",
    "    cos = np.dot(a_query, a_doc)/(np.linalg.norm(a_query)*np.linalg.norm(a_doc))\n",
    "    \n",
    "    # Put the result in the dictionary\n",
    "    scores_dictionary[doc_index] = cos\n",
    "    \n",
    "    # Update the heap\n",
    "    heapq.heappush(heap, cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1104543",
   "metadata": {},
   "source": [
    "Top_k!! funzionerà? sono stanca per scrivere in Inglese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02704ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "k = 10\n",
    "\n",
    "# Get the ordered list of top_k scores from the heap\n",
    "top_k = heapq.nlargest(k, heap)\n",
    "\n",
    "# Initialize list of top documents \n",
    "top_k_docs = list()\n",
    "\n",
    "# Fill list of top documents\n",
    "for i in range(len(top_k)):\n",
    "    doc_index = list(scores_dictionary.keys())[list(scores_dictionary.values()).index(top_k[i])]\n",
    "    top_k_docs.append(doc_index)\n",
    "    del scores_dictionary[doc_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7cf4c",
   "metadata": {},
   "source": [
    "Now for the result, I can't figure out how to extract them without a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anche su internet h cercato ma mi viene solo così.\n",
    "df_results = df.iloc[top_k_docs][[\"title\", \"description\", \"city\", \"url\"]]\n",
    "df_results.index = list(range(1, k+1))\n",
    "\n",
    "df_results[\"similarity\"] = [x for x in top_k]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
