{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6d3132-179b-4dfa-a1e2-b70ca28981da",
   "metadata": {},
   "source": [
    "# 1.1. Get the list of animes   \n",
    "We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to an anime's url.\n",
    "\n",
    "#1.2. Crawl animes    \n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the stopping point. More details in Important (2).\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b80b71-6561-43ff-af99-5c4a3d2abe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/valentinosacco/opt/anaconda3/lib/python3.8/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/valentinosacco/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "# Install BeautifulSoup, this will be needed to crawl the web\n",
    "!pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6617fc74-876e-4e74-b414-b87866eaa56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import asyncio, this will be needed to perform asynchronous operations\n",
    "import asyncio\n",
    "# HTTP Requests library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing multiprocessing to assign operations to threadpools\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "# Importing this to create necessary directories\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0e11496b-65ec-42d7-9147-acade3ec3566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Defining the amount of cores available for the process to use. If this slows your machine too much, hardcode it. \n",
    "EX: AVAILABLE_CORES = 4\n",
    "'''\n",
    "AVAILABLE_CORES = multiprocessing.cpu_count()\n",
    "print(AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d79aa43d-d57e-433c-b6e0-11797258b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function performs an HTTP Get Request to MyAnimeList and places its results in a given array.\n",
    "Params: \n",
    "    [index] : Simply the page index. Sets up the url for pagination and defines where the page will be placed inside [destination_array]\n",
    "    [destination_array] : where the retrieved page will be stored. The result will be placed in index [index]\n",
    "'''\n",
    "def fetch_page(index, destination_array):\n",
    "    destination_array[index] = requests.get(f\"https://myanimelist.net/topanime.php{'?limit={}'.format(50*index) if(index > 0) else ''}\")\n",
    "    \n",
    "'''\n",
    "Finds all URL contained in a MyAnimeList top animes page, then substitutes them to the starting page inside [pages] array.\n",
    "Params: \n",
    "    [page]  : MyAnimeList's Top Animes HTML Page\n",
    "    [pages] : Array containing all the pages. \n",
    "'''\n",
    "def fetch_urls_in_page(page, pages):\n",
    "    # Defining an html parser\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # Find all URLs\n",
    "    animeUrls = soup.find_all(\"a\", class_=\"hoverinfo_trigger fl-l ml12 mr8\", id=lambda x: x and x.startswith('#area'), href=True)\n",
    "    animeUrls = [a['href'] for a in animeUrls]\n",
    "    # Substitues starting page with its URLs\n",
    "    pages[pages.index(page)] = animeUrls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0e41284a-c8be-44b6-a87d-551b2cf0c773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fetching the pages!\n",
      "Going to fetch urls\n",
      "Done fetching urls\n"
     ]
    }
   ],
   "source": [
    "# Defining pages variables based on how many pages we want to retrieve\n",
    "pages = [None] * 400\n",
    "pages_num = range(0,400)\n",
    "\n",
    "# Initializing ThreadPools \n",
    "pool = ThreadPool(AVAILABLE_CORES)\n",
    "\n",
    "# Crawl Top Animes pages \n",
    "pool.map(lambda num : fetch_page(num, pages), pages_num)   \n",
    "print(\"Done fetching the pages!\")\n",
    "\n",
    "print(\"Going to fetch urls\")\n",
    "# Scraping all URLs present in the crawled pages\n",
    "pool.map(lambda page : fetch_urls_in_page(page, pages), pages)\n",
    "print(\"Done fetching urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1529e55a-eee8-4254-8862-9fd738978eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Performs a GET Request on a given [url] and saves its results as an HTML inside a folder called \"page_[folder]\".\n",
    "The HTML file will be named \"article_[index].html\"\n",
    "'''\n",
    "def fetch_anime_and_parse_html(url, folder, index):\n",
    "    # Get current page\n",
    "    req = requests.get(url)\n",
    "    # MyAnimeList might refuse to respond to large amount of requests, if this happens, we need to stop the process\n",
    "    if(req.status_code != 200) : \n",
    "        raise Exception(f\"My anime list has closed the connection.\\nComplete the captcha and restart the process.\\nCurrent Page was : {index}\")\n",
    "    # Define page's absolute destination path\n",
    "    _directory_path = f\"{pathlib.Path().resolve()}/dataset/page_{folder}\"\n",
    "    #Â Check if current page's destination folder exists... if not, create it!\n",
    "    Path(_directory_path).mkdir(parents=True, exist_ok=True)\n",
    "    # Write the html file in the destination directory.\n",
    "    with open(f\"{_directory_path}/article_{index}.html\", 'w') as file:\n",
    "        file.write(req.text)\n",
    "    \n",
    "\n",
    "'''\n",
    "Assigns fetching to all available threads and calls (fetch_anime_and_parse_html) with given [folderNumber]\n",
    "'''\n",
    "def fetch_animes_and_save_file(urls, folderNumber):\n",
    "    pool = ThreadPool(AVAILABLE_CORES)\n",
    "    pool.map(lambda url : fetch_anime_and_parse_html(url, folderNumber, (50*(folderNumber-1)) + urls.index(url) +1), urls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b7b4e3c0-1652-41fb-9d89-a450b148f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch animes for every requested page\n",
    "\n",
    "'''\n",
    "Here we fetch and save animes in html files. \n",
    "Starting_page defines from which page you want to resume the process. (It works as an index)\n",
    "\n",
    "EX: \n",
    "    to start from scratch:\n",
    "        starting_page = 0\n",
    "    if you want to start from the 10th page:\n",
    "        starting_page = 9\n",
    "    if you want to set 200 as an upper bound:\n",
    "        last_page = 199   \n",
    "'''\n",
    "starting_page = 384\n",
    "last_page = len(pages)\n",
    "pages_to_process = pages[starting_page:]\n",
    "for i in range(0, len(pages_to_process)) : \n",
    "    fetch_animes_and_save_file(pages_to_process[i], starting_page+i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec62a8-75a4-41ac-b5a3-afd55718946c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
