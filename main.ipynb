{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e8c9f1",
   "metadata": {},
   "source": [
    "# Homework 3 - What is the best anime in the world?\n",
    "--- \n",
    "\n",
    "Word provided by :\n",
    " - Valentino Sacco 1949934\n",
    " - Stephanie Tahtouh 1944339\n",
    " - Camilla Savarese 1637288\n",
    " - Anthony Giusti 1992108\n",
    "\n",
    "NB. To facilitate notebook comprehension, we've extracted all functions into three main files:\n",
    " - `./scraping_utils.py`\n",
    " - `./indexing_utils.py`\n",
    " - `./query_utils.py`\n",
    "\n",
    "All functions have been *documented*, so if you have questions about what they do, just *hover with your mouse on the function*!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d4741",
   "metadata": {},
   "source": [
    "*Used libraries:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b80b71-6561-43ff-af99-5c4a3d2abe5d",
   "metadata": {
    "id": "35b80b71-6561-43ff-af99-5c4a3d2abe5d"
   },
   "outputs": [],
   "source": [
    "# Install BeautifulSoup, this will be needed to crawl the web\n",
    "#!pip3 install beautifulsoup4\n",
    "#!pip3 install tqdm\n",
    "#!pip3 install nltk\n",
    "#!pip3 install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e8886",
   "metadata": {},
   "source": [
    "*Notebook Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6617fc74-876e-4e74-b414-b87866eaa56c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6617fc74-876e-4e74-b414-b87866eaa56c",
    "outputId": "fb1a68cb-193f-43dd-bc31-7bbe06adf54c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Valentino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Valentino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if this returns an error on tabulate, install it by running the cell above.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import asyncio, this will be needed to perform asynchronous operations\n",
    "import os\n",
    "import asyncio\n",
    "# Importing multiprocessing to assign operat+ions to threadpools\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "# Importing this to create necessary directories\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from joblib import Parallel, delayed\n",
    "import json \n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import scraping_utils\n",
    "import indexing_utils\n",
    "import query_utils\n",
    "\n",
    "'''if this returns an error on tabulate, install it by running the cell above.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184835c",
   "metadata": {},
   "source": [
    "Since many processes in this homework took a lot of computational time, we've decided to do most of the work using **multiprocessing techniques**.   \n",
    "here is defined our global thread pool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e11496b-65ec-42d7-9147-acade3ec3566",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e11496b-65ec-42d7-9147-acade3ec3566",
    "outputId": "5b522a1f-a5ab-4699-9a1e-dcbf7f954866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Defining the amount of cores available for the process to use. If this slows your machine too much, hardcode it. \n",
    "EX: AVAILABLE_CORES = 4\n",
    "'''\n",
    "AVAILABLE_CORES = multiprocessing.cpu_count()\n",
    "print(AVAILABLE_CORES)\n",
    "\n",
    "# Initializing ThreadPools \n",
    "pool = ThreadPool(AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72992a6",
   "metadata": {},
   "source": [
    "## Data Collection \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d3132-179b-4dfa-a1e2-b70ca28981da",
   "metadata": {
    "id": "3c6d3132-179b-4dfa-a1e2-b70ca28981da"
   },
   "source": [
    "### 1.1. Get the list of animes   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88d6ac",
   "metadata": {},
   "source": [
    "Here we collect urls for the requested 400 pages of the top animes section.   \n",
    "\n",
    "In order to do so, we first fetch the first 400 pages and store them into an array,   \n",
    "In particular we use   \n",
    "```pool.map(lambda page : scraping_utils.fetch_urls_in_page(page, pages), pages)```   \n",
    "to assign the process to many threads\n",
    "\n",
    "then, for each page we scrape all anime URLs and save them in a txt file but also store them in that very same array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e41284a-c8be-44b6-a87d-551b2cf0c773",
   "metadata": {
    "id": "0e41284a-c8be-44b6-a87d-551b2cf0c773",
    "outputId": "68ad8658-c64b-4783-ee62-3e252188b70e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fetching the pages!\n",
      "Going to fetch urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fetching urls\n"
     ]
    }
   ],
   "source": [
    "# Defining pages variables based on how many pages we want to retrieve\n",
    "pages = [None] * 400\n",
    "pages_num = range(0,400)\n",
    "\n",
    "# Crawl Top Animes pages\n",
    "pool.map(lambda num : scraping_utils.fetch_page(num, pages), pages_num)   \n",
    "\n",
    "print(\"Done fetching the pages!\")\n",
    "print(\"Going to fetch urls\")\n",
    "# Scraping all URLs present in the crawled pages\n",
    "pool.map(lambda page : scraping_utils.fetch_urls_in_page(page, pages), pages)\n",
    "\n",
    "print(\"Done fetching urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d4c6a",
   "metadata": {},
   "source": [
    "After fetching all the URLs we need, we proceed to save them in a TXT file, which can be found under:   \n",
    "\n",
    "*./dataset/anime_urls.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c5f5d3-89cf-4b8c-bcc7-4f7f71c7df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written 19139 lines\n",
      "Saved all URLs in ./dataset/anime_urls.txt\n"
     ]
    }
   ],
   "source": [
    "saved_file = scraping_utils.save_urls_to_txt(pages)\n",
    "if(saved_file):\n",
    "    print(\"Saved all URLs in ./dataset/anime_urls.txt\")\n",
    "else :\n",
    "    print(\"Couldn't save URLs in txt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3100b3",
   "metadata": {},
   "source": [
    "## 1.2. Crawl animes\n",
    "\n",
    "After obtaining all urls we proceed to fetch each url's html and save it in a file under its relative page directory.  \n",
    "All html can be found under :\n",
    "`./dataset/pages_*/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4e3c0-1652-41fb-9d89-a450b148f9bf",
   "metadata": {
    "id": "b7b4e3c0-1652-41fb-9d89-a450b148f9bf"
   },
   "outputs": [],
   "source": [
    "# Fetch animes for every requested page\n",
    "'''\n",
    "Here we fetch and save animes in html files. \n",
    "Starting_page defines from which page you want to resume the process. (It works as an index)\n",
    "\n",
    "EX: \n",
    "    to start from scratch:\n",
    "        starting_page = 0\n",
    "    if you want to start from the 10th page:\n",
    "        starting_page = 9\n",
    "    if you want to set 200 as an upper bound:\n",
    "        last_page = 199   \n",
    "'''\n",
    "starting_page = 384\n",
    "last_page = len(pages)\n",
    "pages_to_process = pages[starting_page:]\n",
    "for i in range(0, len(pages_to_process)) : \n",
    "\n",
    "    '''\n",
    "    This functions uses Multiprocessing to assign to given amount of threads the fetching\n",
    "    and saving progress of all given urls into HTMLs.\n",
    "    '''\n",
    "    scraping_utils.fetch_animes_and_save_file(pages_to_process[i], starting_page+i+1, AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a32640",
   "metadata": {},
   "source": [
    "Since every html page contains **metadatas** in order to retrieve the URL of each anime, instead of reading everytime the txt, we extract the url directly from the html page.\n",
    "\n",
    "Here's an example on the output of the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c07f17-9e3c-4fdf-9e10-fc547c2a70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\n"
     ]
    }
   ],
   "source": [
    "with open('./dataset/page_1/article_1.html', 'r', encoding=\"utf-8\") as f:\n",
    "    url = scraping_utils.extract_url_from_html( f.read() )\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iH5KRuRjEEqJ",
   "metadata": {
    "id": "iH5KRuRjEEqJ"
   },
   "source": [
    "## 1.3 Parse downloaded pages\n",
    "---\n",
    "\n",
    "The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "- Anime Name (to save as animeTitle): String.  \n",
    "- Anime Type (to save as animeType): String.   \n",
    "- Number of episode (to save as animeNumEpisode): Integer.   \n",
    "- Release and End Dates of anime (to save as releaseDate and endDate): Convert both release and end date into datetime format.     \n",
    "- Number of members (to save as animeNumMembers): Integer.    \n",
    "- Score (to save as animeScore): Float.   \n",
    "- Users (to save as animeUsers): Integer    \n",
    "- Rank (to save as animeRank): Integer.    \n",
    "- Popularity (to save as animePopularity): Integer.    \n",
    "- Synopsis (to save as animeDescription): String.    \n",
    "- Related Anime (to save as animeRelated): Extract all the related animes, but only keep unique       values and those that have a hyperlink associated to them. List of strings.        \n",
    "- Characters (to save as animeCharacters): List of strings.         \n",
    "- Voices (to save as animeVoices): List of strings.     \n",
    "- Staff (to save as animeStaff): Include the staff  name and their responsibility/task in a list of lists.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "CfWrkzqgr4jI",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CfWrkzqgr4jI",
    "outputId": "0ecb1a4d-1899-42df-e944-861ab379280e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19124it [24:57, 12.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# find all files into directories\n",
    "matches = pathlib.Path(\"./dataset\").glob(\"**/*.html\")\n",
    "# multiprocess parsing every html into a tsv\n",
    "result = Parallel(n_jobs=AVAILABLE_CORES)(delayed(scraping_utils.extract_informations_from_anime_html)(path) for path in tqdm(matches))\n",
    "#pool.map(scraping_utils.extract_informations_from_anime_html, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be173e",
   "metadata": {},
   "source": [
    "Here we first instance a path generator that performs a lookup on the `./dataset` folder to find every html file.\n",
    "\n",
    "Then, to speed up the process as possible, we make use of the Parrallel library, which allows us to divide into many parallel jobs the process of parsing every html into a tsv with the informations we are looking for.\n",
    "\n",
    "With this code, computing more than 19,000 html files takes only 24 minutes.\n",
    "\n",
    "To see how we parse every html into a tsv, you can find the function `extract_informations_from_anime_html` inside the `scraping_utils` module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YFSv5Rn-t8Dd",
   "metadata": {
    "id": "YFSv5Rn-t8Dd"
   },
   "source": [
    "# 2. Search Engine   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc5e08",
   "metadata": {},
   "source": [
    "To preprocess every tsv we computed, we made use of the nltk library as requested. \n",
    "In particular our steps of preprocessing are :\n",
    "\n",
    "**Removing stopwords**   \n",
    "**Removing punctuation**   \n",
    "**Stemming**\n",
    "\n",
    "It's important to note that our punctuation processing is custom and does not remove complex words like \"full-metal\" but it translates them into \"fullmetal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee66ae4d-a4a8-407b-bc99-0d72e8a397c4",
   "metadata": {
    "id": "ee66ae4d-a4a8-407b-bc99-0d72e8a397c4"
   },
   "outputs": [],
   "source": [
    "tsv_matches = pathlib.Path(\"./tsv_dataset\").glob(\"*.tsv\")\n",
    "for match in tsv_matches:\n",
    "    indexing_utils.preprocess_tsv(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa714a88",
   "metadata": {},
   "source": [
    "Every preprocessed tsv gets parsed into a json for performance reasons and can be found under the path    \n",
    "`./preprocessed_dataset/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77709011-1b23-4c2d-a74b-73fef207e750",
   "metadata": {
    "id": "77709011-1b23-4c2d-a74b-73fef207e750"
   },
   "source": [
    "## 2.1. Conjunctive query   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03429c70",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1.1 Create your index!   \n",
    "\n",
    "- Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3262a",
   "metadata": {},
   "source": [
    "For performance reasons we've decided to use json as our vocabulary format. \n",
    "We've also decided to *multiprocess the hydration of the vocabulary* and therefore did not chosse to assign a sequential integer as our term id.\n",
    "Instead, we've decided to use hashcoding as a reliable function to define every word's _id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecb923ff-9dc5-4c49-a0af-5219b8db88c2",
   "metadata": {
    "id": "ecb923ff-9dc5-4c49-a0af-5219b8db88c2"
   },
   "outputs": [],
   "source": [
    "preprocessed_files = pathlib.Path(\"./preprocessed_dataset\").glob(\"*.json\")\n",
    "indexing_utils.hydrate_vocabulary_from_files(preprocessed_files, AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73113afc",
   "metadata": {},
   "source": [
    "Our resulting vocabulary can be found under :   \n",
    "`./vocabulary.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0978e1c",
   "metadata": {},
   "source": [
    "### Our first Inverted Index\n",
    "\n",
    "Below we create our first inverted index.\n",
    "Making use of the previously created vocabulary, we assign to every word in a document's Synopsis a record containing as value every document id of the documents containing that specific word.\n",
    "\n",
    "The index can be found under `./indexes/synopsis_index.json`\n",
    "\n",
    "N.B.: The function used below as been written both using multiprocessing and to be reused for the creation of the next inverted index. Feel free to read its documentation by hovering with your mouse of by looking inside `./indexing_utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a5f1ea-d86b-46c7-b862-e757ac692f23",
   "metadata": {
    "id": "64a5f1ea-d86b-46c7-b862-e757ac692f23",
    "outputId": "f65c59e9-efdb-4d38-bf46-03288c2738bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1------------starting hydrating inverted index-------------\n",
      "1------------inverted index not yet existant---------------\n",
      "1------------reading vocabulary----------------------------\n",
      "1------------proceeding to multiprocess inputs-------------\n",
      "1------------finished to multiprocess inputs---------------\n",
      "1------------finished hydrating inverted index-------------\n",
      "1------------dumping inverted index -----------------------\n",
      "1------------dumped inverted index to path-----------------\n",
      "./indexes/synopsis_index.json\n"
     ]
    }
   ],
   "source": [
    "inverted_index_path = './indexes/synopsis_index.json'\n",
    "vocabulary_path = './vocabulary.json'\n",
    "Path(\"./indexes\").mkdir(parents=True, exist_ok=True)\n",
    "preprocessed_files = pathlib.Path(\"./preprocessed_dataset\").glob(\"*.json\")\n",
    "indexing_utils.hydrate_synopsys_inverted_index_with_given_files_and_vocabulary(inverted_index_path, preprocessed_files, vocabulary_path, AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a7514",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query\n",
    "\n",
    "In order to execute the query we allow the user to provide us a string!\n",
    "To see how the query is evaluated, you can find the code inside `./query_utils.py`   \n",
    "Don't worry about typos, if you misspell a word, the function will tell you to check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d85e08-4a75-4dd7-9779-b14dec116fd7",
   "metadata": {
    "id": "b8d85e08-4a75-4dd7-9779-b14dec116fd7",
    "outputId": "699e026f-b6fc-462d-8d5f-41757fb7fdd3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Type in your query : \n",
      " saiyan race\n",
      "Results for \"saiyan race\" : 4 | elapsed time: 0.1091 seconds\n",
      "| Title                                                   | Description   | Url                                                                                       |\n",
      "|---------------------------------------------------------+---------------+-------------------------------------------------------------------------------------------|\n",
      "| Dragon Ball Super: Broly                                | Forty-on..    | https://myanimelist.net/anime/36946/Dragon_Ball_Super__Broly                              |\n",
      "| Dragon Ball Z Special 1: Tatta Hitori no Saishuu Kessen | Bardock,..    | https://myanimelist.net/anime/986/Dragon_Ball_Z_Special_1__Tatta_Hitori_no_Saishuu_Kessen |\n",
      "| Dragon Ball Kai                                         | Five yea..    | https://myanimelist.net/anime/6033/Dragon_Ball_Kai                                        |\n",
      "| Dragon Ball Z                                           | Five yea..    | https://myanimelist.net/anime/813/Dragon_Ball_Z                                           |\n"
     ]
    }
   ],
   "source": [
    "query_utils.take_simple_query_from_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617cf76-233f-4f9f-ab83-1dea02c27dba",
   "metadata": {
    "id": "1617cf76-233f-4f9f-ab83-1dea02c27dba"
   },
   "source": [
    "## 2.2.1) Our Second Inverted index\n",
    "\n",
    "Here we create our second inverted index, considering the tfidf score of each word.    \n",
    "Let's explain what a **tfidf** score is:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$tfidf$ is the product of **$tf$**, which is the **term frequency factor of a word inside a document**:        \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$tf$ = (# of term occurences in the document) $/$ (# of words in the document)    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$idf$ = $log($(# of docs) $/$ (# of docs containing current word)$)$\n",
    "\n",
    "This results in something like this:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "}\n",
    "```\n",
    "\n",
    "The index can be found under  `'./indexes/tf_idf_synopsis_index.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b18dc9-c8e6-40a1-98ee-2c35ad8584ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63b18dc9-c8e6-40a1-98ee-2c35ad8584ec",
    "outputId": "17ff923d-4efb-4125-e272-6c4dc3f6c00c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1------------starting hydrating inverted index-------------\n",
      "1------------inverted index not yet existant---------------\n",
      "1------------reading vocabulary----------------------------\n",
      "1------------proceeding to multiprocess inputs-------------\n",
      "1------------finished to multiprocess inputs---------------\n",
      "1------------finished hydrating inverted index-------------\n",
      "1------------dumping inverted index -----------------------\n",
      "1------------dumped inverted index to path-----------------\n",
      "./indexes/tf_idf_synopsis_index.json\n"
     ]
    }
   ],
   "source": [
    "inverted_index_path = './indexes/tf_idf_synopsis_index.json'\n",
    "idf_source_index_path = './indexes/synopsis_index.json'\n",
    "vocabulary_path = './vocabulary.json'\n",
    "Path(\"./indexes\").mkdir(parents=True, exist_ok=True)\n",
    "preprocessed_files = pathlib.Path(\"./preprocessed_dataset\").glob(\"*.json\")\n",
    "tot_amount_of_documents = 19125\n",
    "want_idf = True\n",
    "\n",
    "'''\n",
    "this function compiles the inverted based on this logic:\n",
    "\n",
    "it uses the first index to find out in how many docs a word is included, \n",
    "it finds the _id of each word via the vocabulary and (since want_idf == True)\n",
    "it computes the tfidf score of each word and saves it in a tuple like so :\n",
    "\n",
    "term_id_2:[(document1, tfIdf_{term,document1})\n",
    "'''\n",
    "indexing_utils.hydrate_synopsys_inverted_index_with_given_files_and_vocabulary(\n",
    "    inverted_index_path,\n",
    "    preprocessed_files,\n",
    "    vocabulary_path,\n",
    "    AVAILABLE_CORES,\n",
    "    want_idf,\n",
    "    tot_amount_of_documents,\n",
    "    idf_source_index_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8f5e7",
   "metadata": {},
   "source": [
    "Here we ask the user to provide us with query and the amount of documents he/she wants to be retrieved. \n",
    "\n",
    "This is how we evaluate the query :\n",
    "\n",
    "First we compute the interception of words sets to find out what documents contain every word the user is looking for.   \n",
    "Then we generate two kind of vectors, a query_vector and many documents_vectors :\n",
    "\n",
    "- we fill the query vector components with 1 if the word corresponding to the component is present in the query, or 0 if it's not present.    \n",
    "- We also fill the documents vectors components with the TFIDF of each term leaving 0 on the components of the words not contained in the doc.    \n",
    "- Then we compute the similarity between the query vector and each one of the document vectors as the cosine:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$cos(theta)$ = $(qd)/(|q||d|)$\n",
    "- We then store the cosines in a heap structure, in order to make sorting more efficient.\n",
    "- We retrieve the top k (in this case, we ask the user to provide $k$ ) scores, and associate them to the corresponding documents, in order to show the correct ranking in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "-hJrZmTczxFX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hJrZmTczxFX",
    "outputId": "bc8869b5-22fc-424e-a93f-61d20771273f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Type in your query : \n",
      " dragon ball super\n",
      "How many results would you like to retrieve?\n",
      " 5\n",
      "Results for \"dragon ball super\" : 7 | elapsed time: 1.425398 seconds\n",
      "| Title                                                        | Description   | Url                                                                                           |   Similarity |\n",
      "|--------------------------------------------------------------+---------------+-----------------------------------------------------------------------------------------------+--------------|\n",
      "| Dragon Ball Super: Super Hero                                | Sequel m..    | https://myanimelist.net/anime/48903/Dragon_Ball_Super__Super_Hero                             |     0.638038 |\n",
      "| Dragon Ball Z: The Real 4-D at Super Tenkaichi Budokai       | Dragon B..    | https://myanimelist.net/anime/42449/Dragon_Ball_Z__The_Real_4-D_at_Super_Tenkaichi_Budokai    |     0.611956 |\n",
      "| Dragon Ball Super                                            | Seven ye..    | https://myanimelist.net/anime/30694/Dragon_Ball_Super                                         |     0.208705 |\n",
      "| Dragon Ball Z: Summer Vacation Special                       | One peac..    | https://myanimelist.net/anime/22695/Dragon_Ball_Z__Summer_Vacation_Special                    |     0.199    |\n",
      "| Dragon Ball GT: Gokuu Gaiden! Yuuki no Akashi wa Suushinchuu | Years af..    | https://myanimelist.net/anime/987/Dragon_Ball_GT__Gokuu_Gaiden_Yuuki_no_Akashi_wa_Suushinchuu |     0.183221 |\n"
     ]
    }
   ],
   "source": [
    "query_utils.take_top_k_of_query_from_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39062bf",
   "metadata": {},
   "source": [
    "# 3. Define a new score!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676f1b2-01bc-4e23-9806-f3b9689fa78d",
   "metadata": {},
   "source": [
    "As our new score, we decided to implement the jaccard similarity algorithm :\n",
    "\n",
    "The jaccard similarity index is computed like so:\n",
    "\\begin{equation*}\n",
    "\\frac {A \\cap B}{A \\cup B}\n",
    "\\end{equation*}\n",
    "\n",
    "Where A is the set based on the query words and B is the set composed by the element we are analyzing. \n",
    "This formula is applied to every component of the documents, resulting in a vector having as components the Jaccard Indexes for every field: this allows us to have a score that instead of only relying on the synopsis, relies also on:\n",
    "\n",
    "- animeTitle \n",
    "- animeDescription \n",
    "- animeRelated \n",
    "- animeCharacters \n",
    "\n",
    "Then, to better adapt the results to the user preferences, we have given a weight to each field of the documents that changes based on the preference inputs received by the user when asking for the query.\n",
    "\n",
    "In the end, since the user is also asked how many results he wants to be retrieved, we compute a max-heap of the scores in order to retrieve the best $k$ k elements and return them to the user.\n",
    "\n",
    "Every function that act in this process can be found inside the query module : `./query_utils.py` feel free to read the documentations to have a better understanding of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74875d",
   "metadata": {},
   "source": [
    "#### Let's say that we are looking for an old dragon ball movie...\n",
    "\n",
    "so we should look for *very few episodes*, a *quite old anime* with *kind of a good fanbase*, we also may be interested in a *good score* and the anime, since it's old, to *not be too popular...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfc2aea-799c-4579-9445-b2a050e21998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Type in your query : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dragon ball\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many results would you like to retrieve?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will now be asked to type in some preferences. \n",
      "Consider the range from -5 to 5, where -5 is the least, 5 is the max and 0 is indifferent.\n",
      "\n",
      "How many episodes do you prefer?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you looking for a newer or an older anime?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you looking for an anime with a large fanbase?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you looking for an anime with a good score?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you looking for a very popular anime?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------------------+--------------------+-------------------------------------------------------------------------------------------+--------------+\n",
      "|    | animeTitle                                              | animeDescription   | url                                                                                       |   similarity |\n",
      "|----+---------------------------------------------------------+--------------------+-------------------------------------------------------------------------------------------+--------------|\n",
      "|  0 | Dragon Ball Z Movie 02: Kono Yo de Ichiban Tsuyoi Yatsu | In his l...        | https://myanimelist.net/anime/895/Dragon_Ball_Z_Movie_02__Kono_Yo_de_Ichiban_Tsuyoi_Yatsu |     1        |\n",
      "|  0 | Dragon Ball GT                                          | Emperor ...        | https://myanimelist.net/anime/225/Dragon_Ball_GT                                          |     0.959212 |\n",
      "|  0 | Dragon Ball Z: Summer Vacation Special                  | One peac...        | https://myanimelist.net/anime/22695/Dragon_Ball_Z__Summer_Vacation_Special                |     0.904487 |\n",
      "|  0 | Dragon Ball Super: Broly                                | Forty-on...        | https://myanimelist.net/anime/36946/Dragon_Ball_Super__Broly                              |     0.898069 |\n",
      "|  0 | Turning Mecard                                          | A Korean...        | https://myanimelist.net/anime/35541/Turning_Mecard                                        |     0.89407  |\n",
      "+----+---------------------------------------------------------+--------------------+-------------------------------------------------------------------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "query_utils.take_biased_query_from_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e4169",
   "metadata": {},
   "source": [
    "##  5 ALGORITHMC QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa1c87",
   "metadata": {},
   "source": [
    "Basically, given a list of integer and positive numbers (they are appointment's durations),we have to find the maximum sum of subsequence where the subsequence contains no element at adjacent positions.\n",
    "We approached the problem using dynamic programming.\n",
    "We create an auxiliary list $L_1$.\n",
    "\n",
    "$L_1[i]$ represents the maximum possible sum for the sub-array staring from index ‘i’ and ending at index ‘N-1’, and $V$ is the initial input of length N.\n",
    "\n",
    "For every index ‘i’, we have two choices:\n",
    "\n",
    "1) Choose the current index:\n",
    "   So that we will have \n",
    "   $L_1[i]$ = $V[i] + L_1[i-2]$\n",
    "\n",
    "\n",
    "2) Skip the current index:\n",
    "   $L_1[i]$ = $L_1[i-1]$\n",
    "   \n",
    "We will choose the path that maximizes our result:  $L_1[i]$ = max(  $V[i] + L_1[i-2]$, $L_1[i-1]$ )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15dfe6",
   "metadata": {},
   "source": [
    "$L_1[N-1]$ is the longest possible duration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d6b887",
   "metadata": {},
   "source": [
    "- Observation 1: In this way the computational time is $O(N)$\n",
    "- Observation 2: We have to take care of the trivial cases, $N=0,1,2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e81a2",
   "metadata": {},
   "source": [
    "This is the implemtented program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many appointment requests are there? --->6\n",
      "Insert a value that correspond to the duration of the appointment  --->30\n",
      "Insert a value that correspond to the duration of the appointment  --->40\n",
      "Insert a value that correspond to the duration of the appointment  --->25\n",
      "Insert a value that correspond to the duration of the appointment  --->50\n",
      "Insert a value that correspond to the duration of the appointment  --->30\n",
      "Insert a value that correspond to the duration of the appointment  --->20\n",
      "[30, 40, 25, 50, 30, 20]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n=int(input(\"How many appointment requests are there? --->\"))\n",
    "l = []\n",
    "for i in range(1,n+1):\n",
    "        x=int(input(\"Insert a value that correspond to the duration of the appointment  --->\"))\n",
    "        l.append(x)\n",
    "        \n",
    "print (l) #our list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdaade0",
   "metadata": {},
   "source": [
    "The function below is used to find the longest possible duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_seq(l):\n",
    "    n=len(l)\n",
    "    \n",
    "    #basic cases\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return l[0]\n",
    "    if n == 2:\n",
    "        return max(l[0], l[1])\n",
    "\n",
    "    l_1 = [0]*n #new list\n",
    "    l_1[0] = l[0]\n",
    "    l_1[1] = max(l[0], l[1])\n",
    "\n",
    "    for i in range(2, n):\n",
    "        l_1[i] = max(l[i]+l_1[i-2], l_1[i-1])\n",
    "\n",
    "    print(\"The longest possible duration is :\" )\n",
    "    print(l_1[-1])\n",
    "\n",
    "    return l_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0b826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest possible duration is :\n",
      "110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30, 40, 55, 90, 90, 110]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819babca",
   "metadata": {},
   "source": [
    "In this way we have found the maximum possible sum, but not the values that allow us to obtain it.\n",
    "We initialize c with the maximum possible duration.\n",
    "To extract the elements we  scroll through the auxiliary list starting from the end:\n",
    "If the element in position i is greater than the next, it means that it is one of the elements that interest us.\n",
    "So we append it in another auxiliary variable, and we subtract that value from our sum.\n",
    "The boolean is needed to indicate the fact that if the element of index i has been taken from the list, the adjacent one must be skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sub(l,l_1):\n",
    "    l_1= l_1[::-1] #invert\n",
    "    l= l[::-1]\n",
    "    c= l_1[0]\n",
    "    sub= []\n",
    "    boolean = True\n",
    "    N=len(l_1)\n",
    "    for i in range(N):\n",
    "        \n",
    "        if i == N-1 and c > 0:\n",
    "            sub.append(l[i])\n",
    "            c-= l[i]\n",
    "        elif not boolean:\n",
    "            boolean = True\n",
    "            continue\n",
    "        elif  i < N-1 and l_1[i] > l_1[i+1] and boolean:\n",
    "            boolean = False\n",
    "            sub.append(l[i])\n",
    "            c-= l[i]\n",
    "    print(\"The selected appointments are:\")\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfb393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest possible duration is :\n",
      "110\n",
      "The selected appointments are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20, 50, 40]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_sub(l,max_seq(l))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copia di Copia di HW3 in progress",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
