{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6d3132-179b-4dfa-a1e2-b70ca28981da",
   "metadata": {
    "id": "3c6d3132-179b-4dfa-a1e2-b70ca28981da"
   },
   "source": [
    "# 1.1. Get the list of animes   \n",
    "We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to an anime's url.\n",
    "\n",
    "#1.2. Crawl animes    \n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the stopping point. More details in Important (2).\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ncB4vPMdCeLb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncB4vPMdCeLb",
    "outputId": "b8efc5fd-295d-4ac3-ff62-853063a86476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ADM-HW3-Group1'...\n",
      "remote: Enumerating objects: 19530, done.\u001b[K\n",
      "remote: Counting objects: 100% (19530/19530), done.\u001b[K\n",
      "remote: Compressing objects: 100% (439/439), done.\u001b[K\n",
      "remote: Total 19530 (delta 19126), reused 19484 (delta 19086), pack-reused 0\n",
      "Receiving objects: 100% (19530/19530), 108.26 MiB | 20.95 MiB/s, done.\n",
      "Resolving deltas: 100% (19126/19126), done.\n",
      "Checking out files: 100% (19127/19127), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/S4b3/ADM-HW3-Group1.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b80b71-6561-43ff-af99-5c4a3d2abe5d",
   "metadata": {
    "id": "35b80b71-6561-43ff-af99-5c4a3d2abe5d",
    "outputId": "ea4e73d2-bd4d-49be-b06d-1f9565338068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\valentino\\anaconda3\\lib\\site-packages (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "# Install BeautifulSoup, this will be needed to crawl the web\n",
    "#!pip3 install beautifulsoup4\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6617fc74-876e-4e74-b414-b87866eaa56c",
   "metadata": {
    "id": "6617fc74-876e-4e74-b414-b87866eaa56c"
   },
   "outputs": [],
   "source": [
    "# Import asyncio, this will be needed to perform asynchronous operations\n",
    "import asyncio\n",
    "# HTTP Requests library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing multiprocessing to assign operations to threadpools\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "# Importing this to create necessary directories\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e11496b-65ec-42d7-9147-acade3ec3566",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e11496b-65ec-42d7-9147-acade3ec3566",
    "outputId": "10762857-f0e4-4461-937b-5ba5da785cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Defining the amount of cores available for the process to use. If this slows your machine too much, hardcode it. \n",
    "EX: AVAILABLE_CORES = 4\n",
    "'''\n",
    "AVAILABLE_CORES = multiprocessing.cpu_count()\n",
    "print(AVAILABLE_CORES)\n",
    "\n",
    "# Initializing ThreadPools \n",
    "pool = ThreadPool(AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79aa43d-d57e-433c-b6e0-11797258b44a",
   "metadata": {
    "id": "d79aa43d-d57e-433c-b6e0-11797258b44a"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function performs an HTTP Get Request to MyAnimeList and places its results in a given array.\n",
    "Params: \n",
    "    [index] : Simply the page index. Sets up the url for pagination and defines where the page will be placed inside [destination_array]\n",
    "    [destination_array] : where the retrieved page will be stored. The result will be placed in index [index]\n",
    "'''\n",
    "def fetch_page(index, destination_array):\n",
    "    destination_array[index] = requests.get(f\"https://myanimelist.net/topanime.php{'?limit={}'.format(50*index) if(index > 0) else ''}\")\n",
    "    \n",
    "'''\n",
    "Finds all URL contained in a MyAnimeList top animes page, then substitutes them to the starting page inside [pages] array.\n",
    "Params: \n",
    "    [page]  : MyAnimeList's Top Animes HTML Page\n",
    "    [pages] : Array containing all the pages. \n",
    "'''\n",
    "def fetch_urls_in_page(page, pages):\n",
    "    # Defining an html parser\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # Find all URLs\n",
    "    animeUrls = soup.find_all(\"a\", class_=\"hoverinfo_trigger fl-l ml12 mr8\", id=lambda x: x and x.startswith('#area'), href=True)\n",
    "    animeUrls = [a['href'] for a in animeUrls]\n",
    "    # Substitues starting page with its URLs\n",
    "    pages[pages.index(page)] = animeUrls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41284a-c8be-44b6-a87d-551b2cf0c773",
   "metadata": {
    "id": "0e41284a-c8be-44b6-a87d-551b2cf0c773",
    "outputId": "68ad8658-c64b-4783-ee62-3e252188b70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fetching the pages!\n",
      "Going to fetch urls\n",
      "Done fetching urls\n"
     ]
    }
   ],
   "source": [
    "# Defining pages variables based on how many pages we want to retrieve\n",
    "pages = [None] * 400\n",
    "pages_num = range(0,400)\n",
    "\n",
    "# Crawl Top Animes pages \n",
    "pool.map(lambda num : fetch_page(num, pages), pages_num)   \n",
    "print(\"Done fetching the pages!\")\n",
    "\n",
    "print(\"Going to fetch urls\")\n",
    "# Scraping all URLs present in the crawled pages\n",
    "pool.map(lambda page : fetch_urls_in_page(page, pages), pages)\n",
    "print(\"Done fetching urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529e55a-eee8-4254-8862-9fd738978eb6",
   "metadata": {
    "id": "1529e55a-eee8-4254-8862-9fd738978eb6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Performs a GET Request on a given [url] and saves its results as an HTML inside a folder called \"page_[folder]\".\n",
    "The HTML file will be named \"article_[index].html\"\n",
    "'''\n",
    "def fetch_anime_and_parse_html(url, folder, index):\n",
    "    # Get current page\n",
    "    req = requests.get(url)\n",
    "    # MyAnimeList might refuse to respond to large amount of requests, if this happens, we need to stop the process\n",
    "    if(req.status_code != 200) : \n",
    "        raise Exception(f\"My anime list has closed the connection.\\nComplete the captcha and restart the process.\\nCurrent Page was : {index}\")\n",
    "    # Define page's absolute destination path\n",
    "    _directory_path = f\"{pathlib.Path().resolve()}/dataset/page_{folder}\"\n",
    "    # Check if current page's destination folder exists... if not, create it!\n",
    "    Path(_directory_path).mkdir(parents=True, exist_ok=True)\n",
    "    # Write the html file in the destination directory.\n",
    "    with open(f\"{_directory_path}/article_{index}.html\", 'w') as file:\n",
    "        file.write(req.text)\n",
    "    \n",
    "\n",
    "'''\n",
    "Assigns fetching to all available threads and calls (fetch_anime_and_parse_html) with given [folderNumber]\n",
    "'''\n",
    "def fetch_animes_and_save_file(urls, folderNumber):\n",
    "    pool = ThreadPool(AVAILABLE_CORES)\n",
    "    pool.map(lambda url : fetch_anime_and_parse_html(url, folderNumber, (50*(folderNumber-1)) + urls.index(url) +1), urls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4e3c0-1652-41fb-9d89-a450b148f9bf",
   "metadata": {
    "id": "b7b4e3c0-1652-41fb-9d89-a450b148f9bf"
   },
   "outputs": [],
   "source": [
    "# Fetch animes for every requested page\n",
    "\n",
    "'''\n",
    "Here we fetch and save animes in html files. \n",
    "Starting_page defines from which page you want to resume the process. (It works as an index)\n",
    "\n",
    "EX: \n",
    "    to start from scratch:\n",
    "        starting_page = 0\n",
    "    if you want to start from the 10th page:\n",
    "        starting_page = 9\n",
    "    if you want to set 200 as an upper bound:\n",
    "        last_page = 199   \n",
    "'''\n",
    "starting_page = 384\n",
    "last_page = len(pages)\n",
    "pages_to_process = pages[starting_page:]\n",
    "for i in range(0, len(pages_to_process)) : \n",
    "    fetch_animes_and_save_file(pages_to_process[i], starting_page+i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iH5KRuRjEEqJ",
   "metadata": {
    "id": "iH5KRuRjEEqJ"
   },
   "source": [
    "1.3 Parse downloaded pages\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "Anime Name (to save as animeTitle): String.  \n",
    "Anime Type (to save as animeType): String.   \n",
    "Number of episode (to save as animeNumEpisode): Integer.   \n",
    "Release and End Dates of anime (to save as releaseDate and endDate): Convert both release and end date into datetime format.     \n",
    "Number of members (to save as animeNumMembers): Integer.    \n",
    "Score (to save as animeScore): Float.   \n",
    "Users (to save as animeUsers): Integer    \n",
    "Rank (to save as animeRank): Integer.    \n",
    "Popularity (to save as animePopularity): Integer.    \n",
    "Synopsis (to save as animeDescription): String.    \n",
    "Related Anime (to save as animeRelated): Extract all the related animes, but only keep unique       values and those that have a hyperlink associated to them. List of strings.        \n",
    "Characters (to save as animeCharacters): List of strings.         \n",
    "Voices (to save as animeVoices): List of strings.     \n",
    "Staff (to save as animeStaff): Include the staff  name and their responsibility/task in a list of lists.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "JdkLl-ssEIQH",
   "metadata": {
    "id": "JdkLl-ssEIQH"
   },
   "outputs": [],
   "source": [
    "## Defining classes for each argument:\n",
    "def extract_element_from_html(html, html_tag, class_name=\"\", attrs= {}) :\n",
    "  # title class_name\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  # Find given content\n",
    "  content = soup.find(html_tag, class_=class_name, attrs= attrs)\n",
    "  # print(f\"Found {html_tag}: {content}\")\n",
    "  return content\n",
    "\n",
    "def extract_element_from_information_content_by_span_text(html, span_text) :\n",
    "  # title class_name\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  # Find given gontent\n",
    "  pads = soup.find_all(\"div\", class_=\"spaceit_pad\", )\n",
    "  for el in pads :\n",
    "    span = el.find('span')\n",
    "    if(span != None and span.text == span_text):\n",
    "      a = el.find('a')\n",
    "      if(a != None): \n",
    "        return a.text.encode('utf8')\n",
    "      contents = el.contents\n",
    "      if(len(contents) >= 2): \n",
    "        return contents[2].strip(\"\\n \")\n",
    "  return \"\"\n",
    "\n",
    "def extract_related_animes(html):\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  subtag = soup.find(\"table\", \"anime_detail_related_anime\")\n",
    "  #print(f\"Found subtag {subtag}\")\n",
    "  related_animes = []\n",
    "  if(subtag != None): \n",
    "    for el in subtag.find_all(\"a\", href=True):\n",
    "      #print(el)\n",
    "      text = el.text.encode('utf8')\n",
    "      if(text not in related_animes):\n",
    "        related_animes.append(text)\n",
    "  return related_animes\n",
    "\n",
    "\n",
    "def extract_text_list_from_soup_and_class_names(soup, html_tag, class_name):\n",
    "  tag_list = soup.find_all(html_tag, class_name)\n",
    "  output = []\n",
    "  for el in tag_list:\n",
    "    text = el.text.encode('utf8')\n",
    "    if(text not in output):\n",
    "      output.append(text)\n",
    "  return output \n",
    "\n",
    "def extract_soups_tag_list(html, html_tag, class_name):\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  output = soup.find_all(html_tag, class_name)\n",
    "  #print(len(output))\n",
    "  return output\n",
    "\n",
    "def parseDate(date, formats, file_path):\n",
    "  for fmt in formats:\n",
    "    try:\n",
    "        return datetime.strptime(date, fmt)\n",
    "    except ValueError:\n",
    "        pass\n",
    "  print(f\"No valid date format found for : {date} on {file_path}\")\n",
    "  return \"\"\n",
    "\n",
    "def extract_informations_from_anime_html(file_path):\n",
    "  with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "    file_path = str(file_path)\n",
    "    html = f.read()\n",
    "    animeTitle = extract_element_from_html(html, \"h1\", \"title-name h1_bold_none\")\n",
    "    animeTitle = \"\" if animeTitle == None else animeTitle.text.encode('utf8')\n",
    "\n",
    "    animeType = extract_element_from_information_content_by_span_text(html, \"Type:\")\n",
    "    animeNumEpisode = extract_element_from_information_content_by_span_text(html, \"Episodes:\")\n",
    "    rel_and_end_dates = extract_element_from_information_content_by_span_text(html, \"Aired:\")\n",
    "\n",
    "    dates = rel_and_end_dates.split(\" to \")\n",
    "    date_formats = [\"%b %d, %Y\", \"%Y\", \"%b %Y\"]\n",
    "\n",
    "    releaseDate = \"\"\n",
    "    if (dates[0] != None) :\n",
    "      parseDate(dates[0], date_formats, file_path)\n",
    "\n",
    "      \n",
    "    endDate = \"\"\n",
    "    if (len(dates) >= 2 and dates[1] != None) :\n",
    "      parseDate(dates[1], date_formats, file_path)\n",
    "\n",
    "    animeNumMembers = \"\"\n",
    "    try : \n",
    "      int(extract_element_from_html(html, \"span\", \"numbers members\").text.encode('utf8').split()[1].replace(',', ''))\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animeNumMembers - {e} on {file_path}\");\n",
    "\n",
    "    animeScore = \"\"\n",
    "    try:\n",
    "      animeScore = float(extract_element_from_html(html, \"div\", \"score-label\").text.encode('utf8'))\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animeScore - {e} on {file_path}\");\n",
    "    animeUsers = \"\"\n",
    "    try: \n",
    "      animeUsers = int(extract_element_from_html(html, \"div\", \"fl-l score\").get('data-user').split()[0].replace(',',''))\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animeUsers - {e} on {file_path}\");\n",
    "    animeRank = \"\"\n",
    "    try: \n",
    "      nimeRank = int(extract_element_from_html(html, \"span\", \"numbers ranked\").text.encode('utf8').split()[1].replace('#', '').replace(',',''))\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animeRank - {e} on {file_path}\");\n",
    "\n",
    "    animePopularity = \"\"\n",
    "    try:\n",
    "      animePopularity = int(extract_element_from_html(html, \"span\", \"numbers popularity\").text.encode('utf8').split()[1].replace('#', '').replace(',',''))\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animePopularity - {e} on {file_path}\");\n",
    "    animeDescription = \"\"\n",
    "    try:\n",
    "      animeDescription = extract_element_from_html(html, \"p\", \"\", {\"itemprop\": \"description\"}).text.encode('utf8')\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animeDescription - {e} on {file_path}\");\n",
    "    animeRelated = extract_related_animes(html)\n",
    "    char_voices_staff_table = extract_soups_tag_list(html, \"div\", \"detail-characters-list clearfix\")\n",
    "    \n",
    "    animeCharacters = []\n",
    "    try: \n",
    "      animeCharacters = extract_text_list_from_soup_and_class_names(char_voices_staff_table[0], \"h3\", \"h3_characters_voice_actors\")\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animeCharacters {e} on {file_path}\")\n",
    "\n",
    "    animeVoices = []\n",
    "    try: \n",
    "      animeVoices = extract_text_list_from_soup_and_class_names(char_voices_staff_table[0], \"td\", \"va-t ar pl4 pr4\")\n",
    "      animeVoices = [voice.strip('\\n').split('\\n')[0].encode('utf8') for voice in animeVoices]\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animeVoices {e} on {file_path}\") \n",
    "    \n",
    "    animeStaff = []\n",
    "    try: \n",
    "      animeStaff = extract_text_list_from_soup_and_class_names(char_voices_staff_table[1], \"td\", \"borderClass\")\n",
    "      animeStaff = [re.split('\\n+', staff) for staff in list(filter(None, [staff.strip('\\n') for staff in animeStaff]))]\n",
    "    except Exception as e :\n",
    "      pass\n",
    "      #print(f\"animeStaff {e} on {file_path}\") \n",
    "    \n",
    "    article_i = re.findall(re.compile('[0-9]+'), file_path.split('/n')[-1])[-1]\n",
    "    inherited_name = f\"anime_{article_i}.tsv\"\n",
    "    #print(inherited_name)\n",
    "    Path(\"./tsv_dataset\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open('./tsv_dataset/{}'.format(inherited_name), 'wt') as out_file:\n",
    "      tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "      tsv_writer.writerow(['animeTitle', animeTitle])\n",
    "      tsv_writer.writerow(['animeType', animeType])\n",
    "      tsv_writer.writerow(['animeNumEpisode', animeNumEpisode])\n",
    "      tsv_writer.writerow(['releaseDate', releaseDate])\n",
    "      tsv_writer.writerow(['endDate', endDate])\n",
    "      tsv_writer.writerow(['animeNumMembers', animeNumMembers])\n",
    "      tsv_writer.writerow(['animeScore', animeScore])\n",
    "      tsv_writer.writerow(['animeUsers', animeUsers])\n",
    "      tsv_writer.writerow(['animeRank', animeRank])\n",
    "      tsv_writer.writerow(['animePopularity', animePopularity])\n",
    "      tsv_writer.writerow(['animeDescription', animeDescription])\n",
    "      tsv_writer.writerow(['animeRelated', animeRelated])\n",
    "      tsv_writer.writerow(['animeCharacters', animeCharacters])\n",
    "      tsv_writer.writerow(['animeVoices', animeVoices])\n",
    "      tsv_writer.writerow(['animeStaff', animeStaff])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09a4ebc0-58c0-4650-9fb2-4ed22e8d8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_informations_from_anime_html('./dataset/page_358/article_17859.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "CfWrkzqgr4jI",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfWrkzqgr4jI",
    "outputId": "7c556083-befa-482a-abda-5f21918400af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find all files into directories\n",
    "matches = pathlib.Path(\"./dataset\").glob(\"**/*.html\")\n",
    "pool.map(extract_informations_from_anime_html, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "Q5Jl6gPdFgo-",
   "metadata": {
    "id": "Q5Jl6gPdFgo-"
   },
   "outputs": [],
   "source": [
    "!git add .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "U2-QCZj4Gau6",
   "metadata": {
    "id": "U2-QCZj4Gau6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git commit -m \"Exported tsv dataset\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BcGRAdv6GfTP",
   "metadata": {
    "id": "BcGRAdv6GfTP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copia di main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
