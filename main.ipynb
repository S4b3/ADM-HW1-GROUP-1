{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6d3132-179b-4dfa-a1e2-b70ca28981da",
   "metadata": {},
   "source": [
    "# 1.1. Get the list of animes   \n",
    "We start from the list of animes to include in your corpus of documents. In particular, we focus on the top animes ever list. From this list we want to collect the url associated to each anime in the list. The list is long and splitted in many pages. We ask you to retrieve only the urls of the animes listed in the first 400 pages (each page has 50 animes so you will end up with 20000 unique anime urls).\n",
    "\n",
    "The output of this step is a .txt file whose single line corresponds to an anime's url.\n",
    "\n",
    "#1.2. Crawl animes    \n",
    "Once you get all the urls in the first 400 pages of the list, you:\n",
    "\n",
    "Download the html corresponding to each of the collected urls.\n",
    "After you collect a single page, immediately save its html in a file. In this way, if your program stops, for any reason, you will not lose the data collected up to the stopping point. More details in Important (2).\n",
    "Organize the entire set of downloaded html pages into folders. Each folder will contain the htmls of the animes in page 1, page 2, ... of the list of animes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b80b71-6561-43ff-af99-5c4a3d2abe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/valentinosacco/opt/anaconda3/lib/python3.8/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/valentinosacco/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "# Install BeautifulSoup, this will be needed to crawl the web\n",
    "!pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6617fc74-876e-4e74-b414-b87866eaa56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import asyncio, this will be needed to perform asynchronous operations\n",
    "import asyncio\n",
    "# HTTP Requests library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing multiprocessing to assign operations to threadpools\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "# Importing this to create necessary directories\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e41284a-c8be-44b6-a87d-551b2cf0c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "# Prepare the pages needed to find all the urls\n",
    "for i in range(0, 351, 50) :\n",
    "    page = requests.get(f\"https://myanimelist.net/topanime.php{'?limit={}'.format(i) if(i > 0) else ''}\")\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    animeUrls = soup.find_all(\"a\", class_=\"hoverinfo_trigger fl-l ml12 mr8\", id=lambda x: x and x.startswith('#area'), href=True)\n",
    "    animeUrls = [a['href'] for a in animeUrls]\n",
    "    pages.append(animeUrls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1529e55a-eee8-4254-8862-9fd738978eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_anime_and_parse_html(url, folder, index):\n",
    "    # Get current page\n",
    "    req = requests.get(url)\n",
    "    # Define page's absolute destination path\n",
    "    _directory_path = f\"{pathlib.Path().resolve()}/dataset/page_{folder}\"\n",
    "    #Â Check if current page's destination folder exists... if not, create it!\n",
    "    Path(_directory_path).mkdir(parents=True, exist_ok=True)\n",
    "    # Write the html file in the destination directory.\n",
    "    with open(f\"{_directory_path}/article_{index}.html\", 'w') as file:\n",
    "        file.write(req.text)\n",
    "    \n",
    "\n",
    "def fetch_animes_and_save_file(urls, folderNumber):\n",
    "    pool = ThreadPool(4)\n",
    "    pool.map(lambda url : fetch_anime_and_parse_html(url, folderNumber, (50*(folderNumber-1)) + urls.index(url) +1), urls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7b4e3c0-1652-41fb-9d89-a450b148f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch animes for every requested page\n",
    "for i in range(0, len(pages)) : \n",
    "    fetch_animes_and_save_file(pages[i], i+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
