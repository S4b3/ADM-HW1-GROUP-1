{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ncB4vPMdCeLb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncB4vPMdCeLb",
    "outputId": "6b743bae-5216-4e88-8b14-b9961c8e9a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ADM-HW3-Group1'...\n",
      "remote: Enumerating objects: 153084, done.\u001b[K\n",
      "remote: Counting objects: 100% (153084/153084), done.\u001b[K\n",
      "remote: Compressing objects: 100% (118228/118228), done.\u001b[K\n",
      "remote: Total 153084 (delta 104460), reused 83465 (delta 34850), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (153084/153084), 152.79 MiB | 24.53 MiB/s, done.\n",
      "Resolving deltas: 100% (104460/104460), done.\n",
      "Checking out files: 100% (57381/57381), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/S4b3/ADM-HW3-Group1.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef7618",
   "metadata": {},
   "source": [
    "Useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b80b71-6561-43ff-af99-5c4a3d2abe5d",
   "metadata": {
    "id": "35b80b71-6561-43ff-af99-5c4a3d2abe5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.8.9\n"
     ]
    }
   ],
   "source": [
    "# Install BeautifulSoup, this will be needed to crawl the web\n",
    "#!pip3 install beautifulsoup4\n",
    "#!pip3 install tqdm\n",
    "#!pip3 install nltk\n",
    "!pip3 install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6617fc74-876e-4e74-b414-b87866eaa56c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6617fc74-876e-4e74-b414-b87866eaa56c",
    "outputId": "fb1a68cb-193f-43dd-bc31-7bbe06adf54c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Valentino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Valentino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if this returns an error on tabulate, install it by running the cell above.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import asyncio, this will be needed to perform asynchronous operations\n",
    "import os\n",
    "import asyncio\n",
    "# Importing multiprocessing to assign operat+ions to threadpools\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "# Importing this to create necessary directories\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from joblib import Parallel, delayed\n",
    "import json \n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import functions\n",
    "\n",
    "'''if this returns an error on tabulate, install it by running the cell above.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e11496b-65ec-42d7-9147-acade3ec3566",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e11496b-65ec-42d7-9147-acade3ec3566",
    "outputId": "5b522a1f-a5ab-4699-9a1e-dcbf7f954866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Defining the amount of cores available for the process to use. If this slows your machine too much, hardcode it. \n",
    "EX: AVAILABLE_CORES = 4\n",
    "'''\n",
    "AVAILABLE_CORES = multiprocessing.cpu_count()\n",
    "print(AVAILABLE_CORES)\n",
    "\n",
    "# Initializing ThreadPools \n",
    "pool = ThreadPool(AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d198dd",
   "metadata": {},
   "source": [
    "# 1.1. Get the list of animes   \n",
    "We got all the anime urls and saved them in anime_urls.txt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc30b1c-a4cf-4fcf-abd9-f4645f7c1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_urls_to_txt(pages_of_urls):\n",
    "    count = 0\n",
    "    with open('./dataset/anime_urls.txt', 'w', encoding='utf-8') as txt_file :\n",
    "        for page in pages_of_urls :\n",
    "            for url in page :\n",
    "                txt_file.write(url + os.linesep)\n",
    "                count+=1\n",
    "    print(f'written {count} lines')\n",
    "    return True\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0eda3",
   "metadata": {},
   "source": [
    "## 1.2. Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e41284a-c8be-44b6-a87d-551b2cf0c773",
   "metadata": {
    "id": "0e41284a-c8be-44b6-a87d-551b2cf0c773",
    "outputId": "68ad8658-c64b-4783-ee62-3e252188b70e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fetching the pages!\n",
      "Going to fetch urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fetching urls\n"
     ]
    }
   ],
   "source": [
    "# Defining pages variables based on how many pages we want to retrieve\n",
    "pages = [None] * 400\n",
    "pages_num = range(0,400)\n",
    "\n",
    "# Crawl Top Animes pages #\n",
    "pool.map(lambda num : functions.fetch_page(num, pages), pages_num)   \n",
    "# Parallel(n_jobs=AVAILABLE_CORES)(delayed(lambda num : functions.fetch_page(num, pages))(num) for num in tqdm(pages_num))\n",
    "print(\"Done fetching the pages!\")\n",
    "print(\"Going to fetch urls\")\n",
    "# Scraping all URLs present in the crawled pages #\n",
    "pool.map(lambda page : functions.fetch_urls_in_page(page, pages), pages)\n",
    "#Parallel(n_jobs=AVAILABLE_CORES)(delayed(lambda page : functions.fetch_urls_in_page(page, pages))(page) for page in tqdm(pages))\n",
    "print(\"Done fetching urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c5f5d3-89cf-4b8c-bcc7-4f7f71c7df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written 19139 lines\n",
      "Saved all URLs in ./dataset/anime_urls.txt\n"
     ]
    }
   ],
   "source": [
    "saved_file = save_urls_to_txt(pages)\n",
    "if(saved_file):\n",
    "    print(\"Saved all URLs in ./dataset/anime_urls.txt\")\n",
    "else :\n",
    "    print(\"Couldn't save URLs in txt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4e3c0-1652-41fb-9d89-a450b148f9bf",
   "metadata": {
    "id": "b7b4e3c0-1652-41fb-9d89-a450b148f9bf"
   },
   "outputs": [],
   "source": [
    "# Fetch animes for every requested page\n",
    "'''\n",
    "Here we fetch and save animes in html files. \n",
    "Starting_page defines from which page you want to resume the process. (It works as an index)\n",
    "\n",
    "EX: \n",
    "    to start from scratch:\n",
    "        starting_page = 0\n",
    "    if you want to start from the 10th page:\n",
    "        starting_page = 9\n",
    "    if you want to set 200 as an upper bound:\n",
    "        last_page = 199   \n",
    "'''\n",
    "starting_page = 384\n",
    "last_page = len(pages)\n",
    "pages_to_process = pages[starting_page:]\n",
    "for i in range(0, len(pages_to_process)) : \n",
    "    functions.fetch_animes_and_save_file(pages_to_process[i], starting_page+i+1, AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c07f17-9e3c-4fdf-9e10-fc547c2a70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\n"
     ]
    }
   ],
   "source": [
    "with open('./dataset/page_1/article_1.html', 'r', encoding=\"utf-8\") as f:\n",
    "    url = functions.extract_url_from_html( f.read() )\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iH5KRuRjEEqJ",
   "metadata": {
    "id": "iH5KRuRjEEqJ"
   },
   "source": [
    "## 1.3 Parse downloaded pages\n",
    "At this point, you should have all the html documents about the animes of interest and you can start to extract the animes informations. The list of information we desire for each anime and their format is the following:\n",
    "\n",
    "Anime Name (to save as animeTitle): String.  \n",
    "Anime Type (to save as animeType): String.   \n",
    "Number of episode (to save as animeNumEpisode): Integer.   \n",
    "Release and End Dates of anime (to save as releaseDate and endDate): Convert both release and end date into datetime format.     \n",
    "Number of members (to save as animeNumMembers): Integer.    \n",
    "Score (to save as animeScore): Float.   \n",
    "Users (to save as animeUsers): Integer    \n",
    "Rank (to save as animeRank): Integer.    \n",
    "Popularity (to save as animePopularity): Integer.    \n",
    "Synopsis (to save as animeDescription): String.    \n",
    "Related Anime (to save as animeRelated): Extract all the related animes, but only keep unique       values and those that have a hyperlink associated to them. List of strings.        \n",
    "Characters (to save as animeCharacters): List of strings.         \n",
    "Voices (to save as animeVoices): List of strings.     \n",
    "Staff (to save as animeStaff): Include the staff  name and their responsibility/task in a list of lists.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-VT6XH7CS_b",
   "metadata": {
    "id": "c-VT6XH7CS_b"
   },
   "outputs": [],
   "source": [
    "# test code on just one single html:\n",
    "functions.extract_informations_from_anime_html('./dataset/page_1/article_1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "CfWrkzqgr4jI",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CfWrkzqgr4jI",
    "outputId": "0ecb1a4d-1899-42df-e944-861ab379280e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19124it [24:57, 12.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# find all files into directories\n",
    "matches = pathlib.Path(\"./dataset\").glob(\"**/*.html\")\n",
    "# multiprocess parsing every html into a tsv\n",
    "result = Parallel(n_jobs=AVAILABLE_CORES)(delayed(functions.extract_informations_from_anime_html)(path) for path in tqdm(matches))\n",
    "#pool.map(extract_informations_from_anime_html, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YFSv5Rn-t8Dd",
   "metadata": {
    "id": "YFSv5Rn-t8Dd"
   },
   "source": [
    "# 2. Search Engine   \n",
    "Now, we want to create two different Search Engines that, given as input a query, return the animes that match the query.   \n",
    "\n",
    "First, you must pre-process all the information collected for each anime by:\n",
    "\n",
    "**Removing stopwords**   \n",
    "**Removing punctuation**   \n",
    "**Stemming**   \n",
    "**Anything else you think it's needed**   \n",
    "\n",
    "For this purpose, you can use the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd640c3-5338-4d82-a62d-89ded05ba0a0",
   "metadata": {
    "id": "fcd640c3-5338-4d82-a62d-89ded05ba0a0",
    "outputId": "4bc641e7-e0ed-44fe-b937-8e547298c891"
   },
   "outputs": [],
   "source": [
    "functions.preprocess_tsv('./tsv_dataset/anime_11794.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee66ae4d-a4a8-407b-bc99-0d72e8a397c4",
   "metadata": {
    "id": "ee66ae4d-a4a8-407b-bc99-0d72e8a397c4"
   },
   "outputs": [],
   "source": [
    "tsv_matches = pathlib.Path(\"./tsv_dataset\").glob(\"*.tsv\")\n",
    "#result = pool.map(functions.preprocess_tsv, tsv_matches)\n",
    "for match in tsv_matches:\n",
    "    functions.preprocess_tsv(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77709011-1b23-4c2d-a74b-73fef207e750",
   "metadata": {
    "id": "77709011-1b23-4c2d-a74b-73fef207e750"
   },
   "source": [
    "2.1. Conjunctive query   \n",
    "For the first version of the search engine, we narrow our interest on the Synopsis of each anime. It means that you will evaluate queries only with respect to the anime's description.\n",
    "\n",
    "2.1.1 Create your index!   \n",
    "Before building the index,\n",
    "\n",
    "- Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecb923ff-9dc5-4c49-a0af-5219b8db88c2",
   "metadata": {
    "id": "ecb923ff-9dc5-4c49-a0af-5219b8db88c2"
   },
   "outputs": [],
   "source": [
    "preprocessed_files = pathlib.Path(\"./preprocessed_dataset\").glob(\"*.json\")\n",
    "functions.hydrate_vocabulary_from_files(preprocessed_files, AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a5f1ea-d86b-46c7-b862-e757ac692f23",
   "metadata": {
    "id": "64a5f1ea-d86b-46c7-b862-e757ac692f23",
    "outputId": "f65c59e9-efdb-4d38-bf46-03288c2738bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1------------starting hydrating inverted index-------------\n",
      "1------------inverted index not yet existant---------------\n",
      "1------------reading vocabulary----------------------------\n",
      "1------------proceeding to multiprocess inputs-------------\n",
      "1------------finished to multiprocess inputs---------------\n",
      "1------------finished hydrating inverted index-------------\n",
      "1------------dumping inverted index -----------------------\n",
      "1------------dumped inverted index to path-----------------\n",
      "./indexes/synopsis_index.json\n"
     ]
    }
   ],
   "source": [
    "inverted_index_path = './indexes/synopsis_index.json'\n",
    "vocabulary_path = './vocabulary.json'\n",
    "Path(\"./indexes\").mkdir(parents=True, exist_ok=True)\n",
    "preprocessed_files = pathlib.Path(\"./preprocessed_dataset\").glob(\"*.json\")\n",
    "functions.hydrate_synopsys_inverted_index_with_given_files_and_vocabulary(inverted_index_path, preprocessed_files, vocabulary_path, AVAILABLE_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d85e08-4a75-4dd7-9779-b14dec116fd7",
   "metadata": {
    "id": "b8d85e08-4a75-4dd7-9779-b14dec116fd7",
    "outputId": "699e026f-b6fc-462d-8d5f-41757fb7fdd3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Type in your query : \n",
      " saiyan race\n",
      "Results for \"saiyan race\" : 4 | elapsed time: 0.1091 seconds\n",
      "| Title                                                   | Description   | Url                                                                                       |\n",
      "|---------------------------------------------------------+---------------+-------------------------------------------------------------------------------------------|\n",
      "| Dragon Ball Super: Broly                                | Forty-on..    | https://myanimelist.net/anime/36946/Dragon_Ball_Super__Broly                              |\n",
      "| Dragon Ball Z Special 1: Tatta Hitori no Saishuu Kessen | Bardock,..    | https://myanimelist.net/anime/986/Dragon_Ball_Z_Special_1__Tatta_Hitori_no_Saishuu_Kessen |\n",
      "| Dragon Ball Kai                                         | Five yea..    | https://myanimelist.net/anime/6033/Dragon_Ball_Kai                                        |\n",
      "| Dragon Ball Z                                           | Five yea..    | https://myanimelist.net/anime/813/Dragon_Ball_Z                                           |\n"
     ]
    }
   ],
   "source": [
    "functions.take_simple_query_from_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608a356-a012-4f61-b337-78150209c92d",
   "metadata": {
    "id": "8608a356-a012-4f61-b337-78150209c92d"
   },
   "source": [
    "2.2) Conjunctive query & Ranking score<br />\n",
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "Find all the documents that contains all the words in the query.   \n",
    "Sort them by their similarity with the query.   \n",
    "Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. You must use a heap data structure (you can use Python libraries) for maintaining the top-k documents.   \n",
    "To solve this task, you will have to use the **tfIdf** score, and the **Cosine similarity**. The field to consider it is still the synopsis. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617cf76-233f-4f9f-ab83-1dea02c27dba",
   "metadata": {
    "id": "1617cf76-233f-4f9f-ab83-1dea02c27dba"
   },
   "source": [
    "2.2.1) Inverted index\n",
    "Your second Inverted Index must be of this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b18dc9-c8e6-40a1-98ee-2c35ad8584ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63b18dc9-c8e6-40a1-98ee-2c35ad8584ec",
    "outputId": "17ff923d-4efb-4125-e272-6c4dc3f6c00c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1------------starting hydrating inverted index-------------\n",
      "1------------inverted index not yet existant---------------\n",
      "1------------reading vocabulary----------------------------\n",
      "1------------proceeding to multiprocess inputs-------------\n",
      "1------------finished to multiprocess inputs---------------\n",
      "1------------finished hydrating inverted index-------------\n",
      "1------------dumping inverted index -----------------------\n",
      "1------------dumped inverted index to path-----------------\n",
      "./indexes/tf_idf_synopsis_index.json\n"
     ]
    }
   ],
   "source": [
    "inverted_index_path = './indexes/tf_idf_synopsis_index.json'\n",
    "idf_source_index_path = './indexes/synopsis_index.json'\n",
    "vocabulary_path = './vocabulary.json'\n",
    "Path(\"./indexes\").mkdir(parents=True, exist_ok=True)\n",
    "preprocessed_files = pathlib.Path(\"./preprocessed_dataset\").glob(\"*.json\")\n",
    "\n",
    "functions.hydrate_synopsys_inverted_index_with_given_files_and_vocabulary(\n",
    "    inverted_index_path,\n",
    "    preprocessed_files,\n",
    "    vocabulary_path,\n",
    "    AVAILABLE_CORES,\n",
    "    True,\n",
    "    19125,\n",
    "    idf_source_index_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "-hJrZmTczxFX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hJrZmTczxFX",
    "outputId": "bc8869b5-22fc-424e-a93f-61d20771273f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Type in your query : \n",
      " dragon ball super\n",
      "How many results would you like to retrieve?\n",
      " 5\n",
      "Results for \"dragon ball super\" : 7 | elapsed time: 1.425398 seconds\n",
      "| Title                                                        | Description   | Url                                                                                           |   Similarity |\n",
      "|--------------------------------------------------------------+---------------+-----------------------------------------------------------------------------------------------+--------------|\n",
      "| Dragon Ball Super: Super Hero                                | Sequel m..    | https://myanimelist.net/anime/48903/Dragon_Ball_Super__Super_Hero                             |     0.638038 |\n",
      "| Dragon Ball Z: The Real 4-D at Super Tenkaichi Budokai       | Dragon B..    | https://myanimelist.net/anime/42449/Dragon_Ball_Z__The_Real_4-D_at_Super_Tenkaichi_Budokai    |     0.611956 |\n",
      "| Dragon Ball Super                                            | Seven ye..    | https://myanimelist.net/anime/30694/Dragon_Ball_Super                                         |     0.208705 |\n",
      "| Dragon Ball Z: Summer Vacation Special                       | One peac..    | https://myanimelist.net/anime/22695/Dragon_Ball_Z__Summer_Vacation_Special                    |     0.199    |\n",
      "| Dragon Ball GT: Gokuu Gaiden! Yuuki no Akashi wa Suushinchuu | Years af..    | https://myanimelist.net/anime/987/Dragon_Ball_GT__Gokuu_Gaiden_Yuuki_no_Akashi_wa_Suushinchuu |     0.183221 |\n"
     ]
    }
   ],
   "source": [
    "functions.take_top_k_of_query_from_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676f1b2-01bc-4e23-9806-f3b9689fa78d",
   "metadata": {},
   "source": [
    "## Let's say that we are looking for an old dragon ball movie...\n",
    "\n",
    "so we should look for *very few episodes*, a *quite old anime* with *kind of a good fanbase*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cfc2aea-799c-4579-9445-b2a050e21998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Type in your query : \n",
      " dragon ball\n",
      "How many results would you like to retrieve?\n",
      " 5\n",
      "You will now be asked to type in some preferences. \n",
      "Consider the range from -5 to 5, where -5 is the least, 5 is the max and 0 is indifferent.\n",
      "\n",
      "How many episodes do you prefer?\n",
      " -5\n",
      "Are you looking for a newer or an older anime?\n",
      " 3\n",
      "Are you looking for an anime with a large fanbase?\n",
      " 3\n",
      "Are you looking for an anime with a good score?\n",
      " 4\n",
      "Are you looking for a very popular anime?\n",
      " 2\n",
      "+----+--------------------------------------------------------------+--------------------+-----------------------------------------------------------------------------------------------+--------------+\n",
      "|    | animeTitle                                                   | animeDescription   | url                                                                                           |   similarity |\n",
      "|----+--------------------------------------------------------------+--------------------+-----------------------------------------------------------------------------------------------+--------------|\n",
      "|  0 | Dragon Ball Z Movie 15: Fukkatsu no \"F\"                      | Earth is...        | https://myanimelist.net/anime/25389/Dragon_Ball_Z_Movie_15__Fukkatsu_no_F                     |     1        |\n",
      "|  0 | Dragon Ball Super                                            | Seven ye...        | https://myanimelist.net/anime/30694/Dragon_Ball_Super                                         |     0.914876 |\n",
      "|  0 | Dragon Ball Super: Broly                                     | Forty-on...        | https://myanimelist.net/anime/36946/Dragon_Ball_Super__Broly                                  |     0.911193 |\n",
      "|  0 | Dragon Ball GT: Gokuu Gaiden! Yuuki no Akashi wa Suushinchuu | Years af...        | https://myanimelist.net/anime/987/Dragon_Ball_GT__Gokuu_Gaiden_Yuuki_no_Akashi_wa_Suushinchuu |     0.899467 |\n",
      "|  0 | Nezha Nao Hai                                                | The film...        | https://myanimelist.net/anime/8783/Nezha_Nao_Hai                                              |     0.870581 |\n",
      "+----+--------------------------------------------------------------+--------------------+-----------------------------------------------------------------------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "functions.take_biased_query_from_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ec731",
   "metadata": {},
   "source": [
    "##  5 ALGORITHMC QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa1c87",
   "metadata": {},
   "source": [
    "Basically, given a list of integer and positive numbers (they are appointment's durations),we have to find the maximum sum of subsequence where the subsequence contains no element at adjacent positions.\n",
    "We approached the problem using dynamic programming.\n",
    "We create an auxiliary list $L_1$.\n",
    "\n",
    "$L_1[i]$ represents the maximum possible sum for the sub-array staring from index ‘i’ and ending at index ‘N-1’, and $V$ is the initial input of length N.\n",
    "\n",
    "For every index ‘i’, we have two choices:\n",
    "\n",
    "1) Choose the current index:\n",
    "   So that we will have \n",
    "   $L_1[i]$ = $V[i] + L_1[i-2]$\n",
    "\n",
    "\n",
    "2) Skip the current index:\n",
    "   $L_1[i]$ = $L_1[i-1]$\n",
    "   \n",
    "We will choose the path that maximizes our result:  $L_1[i]$ = max(  $V[i] + L_1[i-2]$, $L_1[i-1]$ )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15dfe6",
   "metadata": {},
   "source": [
    "$L_1[N-1]$ is the longest possible duration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d6b887",
   "metadata": {},
   "source": [
    "- Observation 1: In this way the computational time is $O(N)$\n",
    "- Observation 2: We have to take care of the trivial cases, $N=0,1,2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e81a2",
   "metadata": {},
   "source": [
    "This is the implemtented program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c22a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many appointment requests are there? --->6\n",
      "Insert a value that correspond to the duration of the appointment  --->30\n",
      "Insert a value that correspond to the duration of the appointment  --->40\n",
      "Insert a value that correspond to the duration of the appointment  --->25\n",
      "Insert a value that correspond to the duration of the appointment  --->50\n",
      "Insert a value that correspond to the duration of the appointment  --->30\n",
      "Insert a value that correspond to the duration of the appointment  --->20\n",
      "[30, 40, 25, 50, 30, 20]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n=int(input(\"How many appointment requests are there? --->\"))\n",
    "l = []\n",
    "for i in range(1,n+1):\n",
    "        x=int(input(\"Insert a value that correspond to the duration of the appointment  --->\"))\n",
    "        l.append(x)\n",
    "        \n",
    "print (l) #our list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdaade0",
   "metadata": {},
   "source": [
    "The function below is used to find the longest possible duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bbfe574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_seq(l):\n",
    "    n=len(l)\n",
    "    \n",
    "    #basic cases\n",
    "\n",
    "    if n == 0:\n",
    "\n",
    "        return 0\n",
    "\n",
    "    if n == 1:\n",
    "\n",
    "        return l[0]\n",
    "\n",
    "    if n == 2:\n",
    "\n",
    "        return max(l[0], l[1])\n",
    "    l_1 = [0]*n #new list\n",
    " \n",
    "\n",
    "    \n",
    "    l_1[0] = l[0]\n",
    "\n",
    "    l_1[1] = max(l[0], l[1])\n",
    "\n",
    "     \n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(2, n):\n",
    "\n",
    "        l_1[i] = max(l[i]+l_1[i-2], l_1[i-1])\n",
    "    print(\"The longest possible duration is :\" )\n",
    "    print(l_1[-1])\n",
    " \n",
    "\n",
    "    return l_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f0b826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest possible duration is :\n",
      "110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[30, 40, 55, 90, 90, 110]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819babca",
   "metadata": {},
   "source": [
    "In this way we have found the maximum possible sum, but not the values that allow us to obtain it.\n",
    "We initialize c with the maximum possible duration.\n",
    "To extract the elements we  scroll through the auxiliary list starting from the end:\n",
    "If the element in position i is greater than the next, it means that it is one of the elements that interest us.\n",
    "So we append it in another auxiliary variable, and we subtract that value from our sum.\n",
    "The boolean is needed to indicate the fact that if the element of index i has been taken from the list, the adjacent one must be skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85fb8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sub(l,l_1):\n",
    "    l_1= l_1[::-1] #invert\n",
    "    l= l[::-1]\n",
    "    c= l_1[0]\n",
    "    sub= []\n",
    "    boolean = True\n",
    "    N=len(l_1)\n",
    "    for i in range(N):\n",
    "        \n",
    "        if i == N-1 and c > 0:\n",
    "            sub.append(l[i])\n",
    "            c-= l[i]\n",
    "        elif not boolean:\n",
    "            boolean = True\n",
    "            continue\n",
    "        elif  i < N-1 and l_1[i] > l_1[i+1] and boolean:\n",
    "            boolean = False\n",
    "            sub.append(l[i])\n",
    "            c-= l[i]\n",
    "    print(\"The selected appointments are:\")\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fdfb393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest possible duration is :\n",
      "110\n",
      "The selected appointments are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20, 50, 40]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_sub(l,max_seq(l))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copia di Copia di HW3 in progress",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
